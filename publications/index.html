<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Sean Bin Yang</title>
    <meta name="author" content="Sean Bin Yang">
    <meta name="description" content="&lt;p&gt; Computer Science, Aalborg University, Denmark&lt;br&gt; Office: 3.2.37, Selma LagerlÃ¶fs Vej 300, DK-9220, Aalborg Ã˜st, Denmark&lt;br&gt; Email: seany@ieee.org&lt;/p&gt;
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;">

    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              
              <!--  -->
              <!-- Blog -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li> -->
              
              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/services/">Services</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/others/">Others</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <div class="post">


  <article>
    <div class="row">
        <div class="col-sm-3">
            
            <img style="width: 200px;border-radius: 10px" src="/assets/img/Sean.png">
            
        </div>
    
        <div class="col-sm-9">
            <h1 class="post-title">
            Sean Bin Yang
            </h1>
            <p class="desc"></p>
          <p><span style="color: gray;">Assistant Professor (Tenure-Track)</span><br>Office: 3.2.37, Selma LagerlÃ¶fs Vej 300, DK-9220, Aalborg Ã˜st, Denmark<br> Email: seany@ieee.org</p>

            <div class="social">
                <div class="contact-icons">
                <a href="https://scholar.google.com/citations?user=oIYY6AQAAAAJ&hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <!-- <a href="https://github.com/Ha0Tang" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> -->
            <a href="http://linkedin.com/in/sean-bin-yang-5970891a2" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://dblp.org/pid/245/0117.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a>
                </div>
            </div>
   
        </div>
    </div>

    <div class="clearfix">
      <p>About:</p>

      <p>
        Sean Bin Yang is currently a Tenure Track Assistant Professor in Department of Computer Science at Aalborg University and also a faculty member in the Center for Data-Intensive System(Daisy) with Prof. ChristianS. Jensen and Prof. Hua Lu. He is working on the research, development, and innovation of Responsible Foundation Model, Spatial-Temporal Data Mining and AI, with a broad range of applications. Especially for Trustworthiness, Fairness, and Eco-Friendliness Representation Learning, focusing on extracting meaningful representation from various data types, including unlabeled, noisy, adversarial, sequential and graph data. Before that, He received his  Ph.D. degrees in Computer Science from Aalborg University, Supervised by Prof. Bin Yang and Prof. Jilin Hu. Aalborg, Denmark. From Jun. 2021 to Dec. 2021, Yang visited Mila-Quebec AI Institute, Canada under the supervision of Associate Prof. Jian Tang. He also served as the Area Chair/SPC/PC for prestigious conferences, including VLDB, KDD, AAAI, IJCAI, CIKM, IJCNN. 
      </p>
      
      <p>
        His research includes:
        <ul>
            <li>Responsible Foundation Model (a.k.a. Representation Learning)</li>
            <li>Spatio-temporal Data Mining</li>
            <li>Vector Database and Quantum Database</li>
            <li>Computer Vision</li>
            <li>Control Theory</li>
        </ul> 
      </p>

    </div>

          <div class="news">
            <h2>News & Events</h2>
            <div class="table-responsive" style="max-height: 20vw">
              <table class="table table-sm table-borderless">

                  <!-- <th scope="row"><strong style="color: red;">Hiring!</strong></th>
                  <td>
                    <strong style="color: red;">We're hiring Postdoc/Ph.D./Master/Intern researchers on Embodied AI, and AIGC (including LLM) for our PKU lab, feel free to reach out to me direclty.</strong>
                  </td>
                </tr>  -->
                 <th scope="row">2025-03</th>
                  <td>
                    ğŸ‰I was invited to serve as a <strong>Shadow PC Member for VLDB2026</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-03</th>
                  <td>
                    ğŸ‰I was invited to serve as a <strong> PC Member for CIKM 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-03</th>
                  <td>
                    ğŸ‰I was invited to serve as an Area Chair (AC) at <strong>IJCNN 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-03</th>
                  <td>
                    We have 1 paper (LLM for Path Representation Learning) accepted to<strong>WWW 2025</strong>.
                  </td>
                </tr> 

                <!-- <th scope="row">2025-03</th>
                  <td>
                    We have 1 paper (Continual Gesture Learning) accepted to <strong>IJCNN 2025</strong>.
                  </td>
                </tr>  -->

                <!-- <th scope="row">2025-03</th>
                  <td>
                    ğŸ‰I was invited to serve as an Area Chair (AC) at <strong>ACM MM 2025</strong>, and we have 1 paper (Accident Warning Agent) accepted to <strong>IV 2025</strong>.
                  </td>
                </tr>  -->

                <!-- <th scope="row">2025-02</th>
                  <td>
                    ğŸ‰I was invited to serve as an Area Chair (AC) for the Large Language Models (LLM) track at <strong>ACL 2025</strong> and a SPC at <strong>IJCAI 2025</strong>, and we have 3 papers including 1 oral (Mamba for Image Compression + 4D Reconstruction + Diffusion Fourier Neural Operator) accepted to <strong>CVPR 2025</strong>.
                  </td>
                </tr>  -->

                <!-- <th scope="row">2025-01</th>
                  <td>
                    We have 1 paper (SAR Automatic Target Recognition) accepted to <strong>TAES 2025</strong>, 1 paper (Person Image Generation) accepted to <strong>TPAMI 2025</strong>, 1 paper (Explainability in MLLMs) accepted to <strong>NAACL 2025 Main Conference</strong>, and 1 paper (Urological Surgical Robots) accepted to <strong>ICRA 2025</strong>. 
                  </td>
                </tr> 

                <th scope="row">2024-12</th>
                  <td>
                    We have 3 papers (Structured Pruning for LLM + FG-SBIR + Hair Transfer via Diffusion Model) accepted to <strong>AAAI 2025</strong> and 1 paper (Efficient Fine-Tuning of LLM) accepted to <strong>ICASSP 2025</strong>.
                  </td>
                </tr>  -->

                <!-- <th scope="row">2024-11</th>
                  <td>
                    We have 1 paper (Virtual Try-On) accepted to <strong>TMM 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-10</th>
                  <td>
                    We have 1 paper (Quantization on Bird's-Eye View Representation) accepted to <strong>WACV 2025</strong> and 1 paper (Semantic Segmentation on Autonomous Vehicles Platform) accepted to <strong>TCAD 2024</strong>.
                  </td>
                </tr> 


                <th scope="row">2024-06</th>
                  <td>
                    ğŸ‰I joined <strong>Peking University</strong> as an Assistant Professor.
                  </td>
                </tr> 

                <th scope="row">2024-04</th>
                  <td>
                    ğŸ‰I received offers from <strong>MIT</strong> and <strong>Harvard University</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-02</th>
                  <td>
                    We have 7 papers (Explanation for ViT + Faithfulness of ViT + Diffusion Policy for Versatile Navigation + Subject-Driven Generation [Final rating: 455] + Diffusion Model for 3D Hand Pose Estimation + Adversarial Learning for 3D Pose Transfer + Efficient Diffusion Distillation [224->235]) accepted to <strong>CVPR 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-01</th>
                  <td>
                    We have 1 paper (Architectural Layout Generation) accepted to <strong>TPAMI 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-12</th>
                  <td>
                    We have 1 paper (Sign Pose Sequence Generation) accepted to <strong>AAAI 2024</strong>.
                  </td>
                </tr> 


                <th scope="row">2023-10</th>
                  <td>
                    ğŸ‰I was elected as one of the <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6" target="_blank">World's Top 2% Scientists in 2023 by Stanford University</a>
                    and we have 4 papers (BEV Perception + Efficient ViT + 3D Motion Transfer + Graph Distillation) accepted to <strong>NeurIPS 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-09</th>
                  <td>
                    We have 1 paper (Practical Blind Image Denoising) accepted to <strong>MIR 2023</strong> and 1 paper (Diffusion Model for HDR Deghosting) accepted to <strong>TCSVT 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-08</th>
                  <td>
                    ğŸ‰I received an offer from <strong>CMU</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-07</th>
                  <td>
                    We have 1 paper (Semantic Image Synthesis) accepted to <strong>TPAMI 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-06</th>
                  <td>
                    We have 1 paper (Visible-Infrared Person Re-ID) accepted to <strong>ICCV 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-05</th>
                  <td>
                    We have 2 papers (Image Restoration Dataset + 3D-Aware Video Generation) accepted to <strong>CVPRW 2023</strong> and 1 paper (3D Face Generation) accepted to <strong>JSTSP 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-04</th>
                  <td>
                    We have 1 paper (Speed-Aware Object Detection) accepted to <strong>ICML 2023</strong>, 2 papers (Lottery Ticket Hypothesis for ViT + Zero-shot Character Recognition) accepted to <strong>IJCAI 2023</strong>, 1 paper (3D Human Pose Estimation) accepted to <strong>PR 2023</strong>, and 1 paper (SAR Target Recognition) accepted to <strong>TGRS 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-03</th>
                  <td>
                    We have 6 papers (HDR Deghosting + Point Cloud Registration + Graph-Constrained House Generation + Mathematical Architecture Design + Text-to-Image Synthesis + Efficient Semantic Segmentation) accepted to <strong>CVPR 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-02</th>
                  <td>
                    We have 3 papers (Camouflaged Object Detection + Brain Vessel Image Segmentation + Cross-View Image Translation) accepted to <strong>ICASSP 2023</strong> and 1 paper (Camouflaged Object Detection) accepted to <strong>TCSVT 2023</strong>.
                  </td>
                </tr> 
               
                <tr>
                  <th scope="row">2023-01</th>
                  <td>
                    We have 1 paper (Semantic Image Synthesis) accepted to <strong>ICLR 2023</strong> and 1 paper (Human Reaction Generation) accepted to <strong>TMM 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-11</th>
                  <td>
                    We have 4 papers (Real-Time Segmentation + Wearable Design + Efficient ViT Training + Text-Guided Image Editing) accepted to <strong>AAAI 2023</strong>, 1 paper accepted (Person Pose and Facial Image Synthesis) to <strong>IJCV 2022</strong>, 1 paper (Salient Object Detection) accepted to <strong>TIP 2022</strong>, and 1 paper (Object Detection Transformer) accepted to <strong>TCSVT 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2022-10</th>
                  <td>
                    We have 1 paper (Sinusoidal Neural Radiance Fields) accepted to <strong>BMVC 2022</strong> and 1 paper (Guided Image-to-Image Translation) accepted to <strong>TPAMI 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-09</th>
                  <td>
                    We have 1 paper (Facial Expression Translation) accepted to <strong>TAFFC 2022</strong> and 1 paper (Ship Detection) accepted to <strong>TGRS 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-07</th>
                  <td>
                     We have 5 papers (Real-Time SR + Video SR + Soft Token Pruning for ViT + 3D-Aware Human Synthesis + Video Semantic Segmentation) accepted to <strong>ECCV 2022</strong>, 1 paper (Gaze Correction and Animation) accepted to <strong>TIP 2022</strong>, and 1 paper (Cross-view Panorama Image Synthesis) accepted to <strong>PR 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-06</th>
                  <td>
                     We have 2 papers (Character Image Restoration + Character Image Denoising) accepted to <strong>ACM MM 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-04</th>
                  <td>
                    We have 1 paper (Real-Time Portrait Stylization) accepted to <strong>IJCAI 2022</strong>, 1 paper (Wide-Context Transformer for Semantic Segmentation) accepted to <strong>TGRS 2022</strong>, and 1 paper (Incremental Learning for Semantic Segmentation) accepted to <strong>TMM 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2022-03</th>
                  <td>
                    We have 5 papers including 1 oral (Text-to-Image Synthesis + 3D Human Pose Estimation + Text-Driven Image Manipulation + 3D Face Modeling + 3D Face Restoration) accepted to <strong>CVPR 2022</strong>, 1 paper (Image Generation) accepted to <strong>TPAMI 2022</strong>, and 1 paper (Cross-View Panorama Image Synthesis) accepted to <strong>TMM 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-12</th>
                  <td>
                    We have 2 papers (Generalized 3D Pose Transfer + Audio-Visual Speaker Tracking) accepted to <strong>AAAI 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-11</th>
                  <td>
                    We have 1 paper (Building Extraction in VHR Remote Sensing Images) accepted to <strong>TIP 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-10</th>
                  <td>
                    We have 3 papers (Cross-View Image Translation + Data-driven 3D Animation + Natural Image Matting) accepted to <strong>BMVC 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-08</th>
                  <td>
                    We have 1 paper (Layout-to-Image Translation) accepted to <strong>TIP 2021</strong> and 1 paper (Unpaired Image-to-Image Translation) accepted to <strong>TNNLS 2021</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2021-07</th>
                  <td>
                    We have 2 papers (Continuous Pixel-Wise Prediction + Unsupervised 3D Pose Transfer) accepted to <strong>ICCV 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-06</th>
                  <td>
                    We have 1 paper (Cross-View Exocentric to Egocentric Video Synthesis) accepted to <strong>ACM MM 2021</strong> and 1 paper (Total Generate) accepted to <strong>TMM 2021</strong>.
                  </td>
                </tr> 

                <th scope="row">2021-05</th>
                  <td>
                    ğŸ‰I received an offer from <strong>ETH Zurich</strong>.
                  </td>
                </tr> 

                <th scope="row">2020-09</th>
                  <td>
                    ğŸ‰I received an offer from <strong>IIAI</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2020-08</th>
                  <td>
                    We have 1 paper (Person Image Generation) accepted to <strong>BMVC 2020</strong>, 2 papers (Semantic Image Synthesis + Unsupervised Gaze Correction and Animation) accepted to <strong>ACM MM 2020</strong>, and 1 paper (Controllable Image-to-Image Translation) accepted to <strong>TIP 2020</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2020-07</th>
                  <td>
                    We have 1 paper (Person Image Generation) accepted to <strong>ECCV 2020</strong>.
                  </td>
                </tr>

                <tr>
                  <th scope="row">2020-05</th>
                  <td>
                    We have 1 paper (Deep Dictionary Learning and Coding) accepted to <strong>TNNLS 2020</strong> and 1 paper (Semantic Segmentation of Remote Sensing Images) accepted to <strong>TGRS 2020</strong>.
                  </td>
                </tr>

                <tr>
                  <th scope="row">2020-02</th>
                  <td>
                    We have 1 paper (Semantic-Guided Scene Generation) accepted to <strong>CVPR 2020</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2019-07</th>
                  <td>
                    We have 1 paper (Keypoint-Guided Image Generation) accepted to <strong>ACM MM 2019</strong>.
                  </td>
                </tr>

                <th scope="row">2019-05</th>
                  <td>
                    ğŸ‰I received an offer from <strong>University of Oxford</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2019-02</th>
                  <td>
                    We have 1 paper (Cross-View Image Translation) accepted to <strong>CVPR 2019</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2018-06</th>
                  <td>
                    We have 1 paper (Hand Gesture-to-Gesture Translation) accepted to <strong>ACM MM 2018</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2018-02</th>
                  <td>
                    We have 1 paper (Monocular Depth Estimation) accepted to <strong>CVPR 2018</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2016-07</th>
                  <td>
                    We have 1 paper (Large Scale Image Retrieval) accepted to <strong>IJCAI 2016</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2015-08</th>
                  <td>
                    We have 1 paper (Gender Classification) accepted to <strong>ACM MM 2015</strong>.
                  </td>
                </tr> -->
                
              </table>
            </div> 
          </div>


<!-- <!-- <div class="lab" style="margin-top: 40px;">
      <h2>Position Openings</h2>
      <div class="box">
            <!-- <p><strong style="color: red;">We are recruiting two postdocs to join our team, focusing on cutting-edge Embodied AI topics (e.g., robotics, perception, and adaptive intelligence).</strong></p> -->

          <!--   <p>For prospective collaborators interested in Embodied AI, and AIGC (including LLM), we are offering multiple positions for Postdoc/Ph.D./Master/Intern researchers. We welcome applicants from diverse disciplines, including Computer Scienceã€Roboticsã€Medicineã€Physicsã€Chemistryã€Biologyã€and more. If you are interested in using AI to solve problems in your field, you are encouraged to apply. Please email me with your self-introduction, the project of interest (including the problem you are trying to solve and how you plan to solve it, being as specific as possible), your transcript, and CV. Send your application to haotang@pku.edu.cn. I'm sorry that I may not be able to respond to every email, but I assure you that your message will stand out if you have a strong research background.</p>

            <p>For Ph.D./Postdoc/Master applicants, we have several openings for domestic students each year. Please reach out at least one year prior to the application deadline.

            For international students, PKU CS offers a variety of programs in English, including <a href="https://twitter.com/HaoTang_ai/status/1847175500558750055/photo/1" target="_blank">Master's</a>, <a href="http://www.studyatpku.com" target="_blank">Ph.D. programs</a>, <a href="https://cs.pku.edu.cn/English/Internationalization/Summer_Winter_Camp1.htm" target="_blank">Summer/Winter Schools</a>, and <a href="https://cs.pku.edu.cn/English/Internationalization/Internship_Program.htm" target="_blank">various other options</a>. Feel free to reach out if you are interested or have any questions.

            For visiting students/undergraduates/research interns/research assistants, we welcome undergraduate and graduate students from all over the world to apply for >6 months research internship. Our visitors/interns have published many top-tier conference/journal papers (e.g., TPAMIã€CVPRã€NeurIPS) and have been admitted to Postdoc/Ph.D./Master programs in prestigious institutions such as MIT, Harvard, Google, University of Toronto, Caltech, ETH ZÃ¼rich, NTU, NUS, and TUM.</p> -->
      <!-- </div> --> -->
      <!-- <h2>Research Lab</h2>
      <div class="box">

            <p>The mission of our research lab is to harness AI to address real-world challenges. Our research priorities include Embodied AI and AIGC.</p>

        <ul>
            <li>Xiaoyuan Wang (Visiting from CMU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Zhenyu Lu (Visiting from CMU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Wenbo Gou (Visiting from CMU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Jun Liu (Visiting from Northeastern University, USAğŸ‡ºğŸ‡¸)</li>
            <li>Zihao Wang (Visiting from UPenn, USAğŸ‡ºğŸ‡¸)</li>
            <li>Yao Gong (Visiting from UPenn, USAğŸ‡ºğŸ‡¸)</li>
            <li>Junjie Zeng (Visiting from UMich, USAğŸ‡ºğŸ‡¸)</li>
            <li>Xiaoyi Liu (Visiting from Washington University in St. Louis, USAğŸ‡ºğŸ‡¸)</li>
            <li>Jingyi Wan (Visiting from University of Cambridge, UKğŸ‡¬ğŸ‡§)</li>
            <li>Xuanyu Lai (Visiting from Imperial College London, UKğŸ‡¬ğŸ‡§)</li>
            <li>Baohua Yin (Visiting from University of Sussex, UKğŸ‡¬ğŸ‡§)</li>
            <li>Zhiguang Han (Visiting from NTU, SingaporeğŸ‡¸ğŸ‡¬)</li>
            <li>Pirzada Suhail (Visiting from IIT Bombay, IndiağŸ‡®ğŸ‡³)</li>
            <li>Zeyu Zhang (Visiting from Australian National University, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Hongpeng Wang (Visiting from University of Sydney, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Zeyu Ren (Visiting from University of Melbourne, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Zhixing Wang (Visiting from University of Malaya, MalaysiağŸ‡²ğŸ‡¾)</li>
            <li>Yuxuan Fan (Visiting from HKUST (Guangzhou), ChinağŸ‡¨ğŸ‡³)</li>
            <li>Nonghai Zhang (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Chenyang Gu (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Keyu Chen (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Di Yu (Visiting from Tsinghua University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Yuxuan Zhang (Visiting from Shanghai Jiao Tong University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Renkai Wu (Visiting from Shanghai Jiao Tong University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Yihua Shao (Visiting from University of Science and Technology Beijing, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Aoming Liang (Visiting from Westlake University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Sifan Li (Visiting from Liaoning University, ChinağŸ‡¨ğŸ‡³)</li>
        </ul>   

        <p><strong>Former members and visitors:</strong> 

        <ul>
            <li>Guillaume Thiry (Intern, now Software Engineer at Google, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Sherwin Bahmani (Intern, now Ph.D. student at University of Toronto, CanadağŸ‡¨ğŸ‡¦)</li>
            <li>Sanghwan Kim (Intern, now Ph.D. student at TUM, GermanyğŸ‡©ğŸ‡ª)</li>
            <li>Alexandros Delitzas (Intern, now Ph.D. student at ETH ZÃ¼rich and Max Planck Institute for Informatics, SwitzerlandğŸ‡¨ğŸ‡­ and GermanyğŸ‡©ğŸ‡ª)</li>
            <li>Jingfeng Rong (Intern, now Ph.D. student at Swiss Finance Institute, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Yitong Xia (Intern from ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Boyan Duan (Intern, now Master at ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Baptiste Chopin (now Postdoc at INRIA, FranceğŸ‡«ğŸ‡·)</li>
            <li>Xiaoyu Yi (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
        </ul>   
   
      </div> -->

      <h2>Teaching</h2>
      <div class="box">   
            <ul>
              <li><a href="https://sean-bin-yang.github.io/AAU" target="_blank">AAU: Data Intensive Systems</a></li>   
              <li>AAU: Data Mining</li>   
            </ul>   

      </div>

      <!-- <h2>International Collaborations</h2>
      <div class="box">

            <p>
            Our lab maintains strong collaborative relationships with several leading international research institutions, including 
              <ul>
                  <li>USAğŸ‡ºğŸ‡¸: MIT, Harvard, Stanford University, CMU, Princeton University, UIUC, UMich, Northeastern University, University of Maryland, University of Texas at Austin, UC Irvine, University of Illinois at Chicago, Illinois Institute of Technology, University of Connecticut, Texas State University, University of Georgia, Clemson University, University of Oregon, College of William & Mary</li>
                  <li>CanadağŸ‡¨ğŸ‡¦: University of Toronto, Simon Fraser University</li>
                  <li>SwitzerlandğŸ‡¨ğŸ‡­: ETH ZÃ¼rich, EPFL</li>
                  <li>UKğŸ‡¬ğŸ‡§: University of Oxford, University of Cambridge, University of Leicester, University of Warwick</li>
                  <li>ItalyğŸ‡®ğŸ‡¹: University of Trento, FBK, Politecnico di Milano, University of Modena e Reggio Emilia</li>
                  <li>GermanyğŸ‡©ğŸ‡ª: TUM, University of WÃ¼rzburg</li>
                  <li>FranceğŸ‡«ğŸ‡·: INRIA, University of Lille</li>
                  <li>FinlandğŸ‡«ğŸ‡®: University of Oulu</li>
                  <li>NetherlandsğŸ‡³ğŸ‡±: TU Delft</li>
                  <li>BelgiumğŸ‡§ğŸ‡ª: KU Leuven</li>
                  <li>BulgariağŸ‡§ğŸ‡¬: INSAIT</li>
                  <li>SingaporeğŸ‡¸ğŸ‡¬: NUS, NTU</li>
                  <li>JapanğŸ‡¯ğŸ‡µ: University of Tokyo, National Institute of Informatics</li>
                  <li>South KoreağŸ‡°ğŸ‡·: Sungkyunkwan University</li>
                  <li>AustraliağŸ‡¦ğŸ‡º: University of Adelaide, ANU, Monash University, University of Technology Sydney</li>
                  <li>UAEğŸ‡¦ğŸ‡ª: IIAI, MBZUAI</li>  
                  <li>HongKongğŸ‡­ğŸ‡°: University of Hong Kong, Hong Kong University of Science and Technology</li>  
              </ul>   
            I am deeply grateful for the opportunities to collaborate with such esteemed institutions and for the valuable contributions they have made to our joint research efforts.

            Additionally, we maintain long-term collaborations with industry, including Google, Meta, Amazon, Cisco, Western Digital, Mercedes-Benz, Xiaohongshu, Alibaba, Tencent, etc, aiming to translate cutting-edge research into practical applications and drive technological advancement.
      </div> -->

    </div>
      
          <div class="publications">
            <h2>Featured Publications</h2>
            <h5>(Including KDD, ICDE, VLDB, IJCAI, TKDE)</h5>
            <p class="post-description"><sup>â€ </sup>My Students or Interns, <sup>*</sup>Corresponding Author(s)</p>
            <ol class="bibliography">

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://kdd.org/kdd2023/index.html" rel="external nofollow noopener" target="_blank">KDD</a></abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/kdd/YangHGYJ23" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">LightPath: Lightweight and Scalable Path Representation Learning</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Sean Bin Yang</em>, Â Jilin Hu, Â Chenjuan Guo, Â Bin Yang, Â Christian S. Jensen </div>

          <div class="periodical">
          
            In <em>KDD</em> 2023, LONG BEACH, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://dl.acm.org/doi/10.1145/3580305.3599415" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Sean-Bin-Yang/LightPath" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

</ol>
          </div>    
    <!-- <a href="https://clustrmaps.com/site/19ncr" title="Visit tracker" rel="external nofollow noopener" target="_blank"><img src="//www.clustrmaps.com/map_v2.png?d=jIdUd0dDYkE8CiqptfhnfiWcZHCc5p62dIsontyW-FQ&amp;cl=ffffff" style="width: 0px;"></a> -->
  </article>

</div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        Â© Copyright 2025 Sean Bin yang. Last updated: Apr 4, 2025.

      
        <script type="text/javascript">
        var sc_project=12830645; 
        var sc_invisible=1; 
        var sc_security="11b4016e"; 
        </script>
        <script type="text/javascript"
        src="https://www.statcounter.com/counter/counter.js"
        async></script>
        <noscript><div class="statcounter"><a title="Web Analytics
        Made Easy - Statcounter" href="https://statcounter.com/"
        target="_blank"><img class="statcounter"
        src="https://c.statcounter.com/12830645/0/11b4016e/1/"
        alt="Web Analytics Made Easy - Statcounter"
        referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
        <!-- End of Statcounter Code -->

      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
